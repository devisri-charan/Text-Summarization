{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using the Hugging Face Transformers library to train a BERT model for summarizing paragraphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Collect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "batch_text_or_text_pairs has to be a list or a tuple (got <class 'pandas.core.series.Series'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\a\\Documents\\Coding\\Machine Learning\\Text Summarization\\textsummarization.ipynb Cell 5\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/a/Documents/Coding/Machine%20Learning/Text%20Summarization/textsummarization.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/a/Documents/Coding/Machine%20Learning/Text%20Summarization/textsummarization.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Tokenize and encode text and summary\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/a/Documents/Coding/Machine%20Learning/Text%20Summarization/textsummarization.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m encoded_data \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mbatch_encode_plus(df[\u001b[39m'\u001b[39;49m\u001b[39mArticle\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/a/Documents/Coding/Machine%20Learning/Text%20Summarization/textsummarization.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                            df[\u001b[39m'\u001b[39;49m\u001b[39mSummary\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/a/Documents/Coding/Machine%20Learning/Text%20Summarization/textsummarization.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                                            padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/a/Documents/Coding/Machine%20Learning/Text%20Summarization/textsummarization.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                            truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/a/Documents/Coding/Machine%20Learning/Text%20Summarization/textsummarization.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                                            max_length\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/a/Documents/Coding/Machine%20Learning/Text%20Summarization/textsummarization.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                                            return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/a/Documents/Coding/Machine%20Learning/Text%20Summarization/textsummarization.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Convert data to PyTorch tensors\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/a/Documents/Coding/Machine%20Learning/Text%20Summarization/textsummarization.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m input_ids \u001b[39m=\u001b[39m encoded_data[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:2765\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2755\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2756\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2757\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2758\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2762\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2763\u001b[0m )\n\u001b[1;32m-> 2765\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   2766\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2767\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2768\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2769\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2770\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2771\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2772\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2773\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2774\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2775\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2776\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2777\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2778\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2779\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2780\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2781\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2782\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2783\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\tokenization_utils_fast.py:416\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_batch_encode_plus\u001b[39m(\n\u001b[0;32m    394\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    395\u001b[0m     batch_text_or_text_pairs: Union[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m     verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    413\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[0;32m    415\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_text_or_text_pairs, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)):\n\u001b[1;32m--> 416\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    417\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch_text_or_text_pairs has to be a list or a tuple (got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(batch_text_or_text_pairs)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    418\u001b[0m         )\n\u001b[0;32m    420\u001b[0m     \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[0;32m    421\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[0;32m    422\u001b[0m         padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m    423\u001b[0m         truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    426\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    427\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: batch_text_or_text_pairs has to be a list or a tuple (got <class 'pandas.core.series.Series'>)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode text and summary\n",
    "encoded_data = tokenizer.batch_encode_plus(df['Article'], \n",
    "                                           df['Summary'], \n",
    "                                           padding=True, \n",
    "                                           truncation=True, \n",
    "                                           max_length=512, \n",
    "                                           return_tensors='pt')\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "input_ids = encoded_data['input_ids']\n",
    "attention_masks = encoded_data['attention_mask']\n",
    "labels = encoded_data['labels']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Split dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_ids, labels, test_size=0.2, random_state=42)\n",
    "train_masks, test_masks, _, _ = train_test_split(attention_masks, input_ids, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Train a Machine Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "\n",
    "# Initialize BERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "\n",
    "# Set up optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 2\n",
    "total_steps = len(X_train) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Train BERT model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i in range(len(X_train)):\n",
    "        input_id = X_train[i].to(device)\n",
    "        attention_mask = train_masks[i].to(device)\n",
    "        label = y_train[i].to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(input_id, attention_mask=attention_mask, labels=label)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Evaluate model performance on testing set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    for i in range(len(X_test)):\n",
    "        input_id = X_test[i].to(device)\n",
    "        attention_mask = test_masks[i].to(device)\n",
    "        output = model(input_id, attention_mask=attention_mask)\n",
    "        y_pred.append(output.logits.item())\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean squared error: {mse}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Deploy model for use in summarizing new paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert new paragraph to PyTorch tensor\n",
    "new_text = 'This is a new paragraph to summarize.'\n",
    "encoded_text = tokenizer.encode_plus(new_text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "input_id = encoded_text['input_ids'].to(device)\n",
    "attention_mask = encoded_text['attention_mask'].to(device)\n",
    "\n",
    "# Use trained model to generate summary\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_id, attention_mask=attention_mask)\n",
    "new_summary = output.logits.item()\n",
    "print(f'Summary: {new_summary}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "646a4c81948903a69c9f4a2df4f57a6bafa622f325d5749ef087c1f06f54ddcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
